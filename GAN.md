15年-16年，主要是解决图片生成问题，图片是像素级，
    比如255*255的大小像素，每个像素的可能又是255
    图像超清化，换脸，图像修复，图像生成漫画，给漫画上色，数据生成 等等
    （原理，和四个算法DCGAN,Pix2Pix,CycleGAN,StarGAN，，Text2img
 1.GAN原理，或者动机
        判别问题的发展：比如在分类，物体检测等进展很大，手段Drpout,Batch Normalization
        生成模型：无中生有的模型，无法利用判别模型的诸多技术
        如何能借助判别模型的优势？
    生成器G，判别器D，最终目标就是生成的仿真数据最后骗过D，得到一个很好的生成器G
 
 2.反卷积(Deconnvolution)（相当于乘以了个转置矩阵
    卷积操作：4*4 经过 3*3的卷积核 变成了 2*2 是在4*4上依次取3*3做 内积 
        一个内积，就是一个结果
        内积可以用矩阵乘法来解释：输入16*1  卷积核4*16  结果4*1，最后重排位2*2
    反卷积()：
       上诉卷积的反操作：就是结果为4*1， 卷积核为4*16  (然后T一下)  结果4*4
       或者是2*2进行panding后，用3*3卷积  得到4*4
    结论：（反卷积的梯度传播，也是先展开成矩阵的形式的
        卷积的正向传播就是反卷积的反向传播
        卷积的反向传播就是反卷积的正向传播
    padding的时候：
        可以边上加，也可以输入矩阵的中间加(Fractional strided)
    
 2.深度卷积对抗生成网络(DCGAN)
    从随即向量中生成真是图像（不同的向量对应不同的真实图像，这样有输入就避免了只生成一张
    如何构建一个新的GAN 网络？
        定义目标（学到一个什么东西，比如学到如何生成一个真实图像，或者真实且和问题匹配的图
        定义生成器G的输入输出（比如输入：随即向量，输出：一张图片
        定义判别器D的输入输出（比如输入：是G输出和现实的图片，输出：是一个类别，Yes or No
        定义G和D的结构
    DCGAN的目标和机构;
        生成真是图像的目标函数是：
            训练判别器D，就是让目标函数变大（越大，区分度越大，就是判别器的判别功能越大，G生成的差
            训练生成器G，就是让目标函数变小（越小，区分度越小，生成的越真实
        生成器G的结构：（输入到输出的过程）
            100的向量->映射和重新排列成4*4*1024的矩阵->经过3个反卷积，长宽不断翻倍，通道数不断缩小
                最后生成64*64*3的图像
        Poolin层用Convolution层替代：
            pooling可以放大和缩小，但是会存在丢失数值的情况，Con就保证了每个都用了
            判别器D上使用strided convolutions, strided是>1
            生成器G上使用的是fractional-strided convolutions, strided是<1的数
        G和D都是使用batch normalization
            帮助解决初始化差的问题
            梯度传播到每一层（梯度传播的比较好，不消失和爆炸
            BN不应用于输入层和输出层
        移除全连接层，使用global pooling(类似Res-net)
        G上除了输出层使用tanh外(输出是0-255，像素)，其他使用Relu
        D上使用LeakyReLU
    应用结果：
        比如：微调输入的随机向量中的某几个值，让图片中某些东西出现或者消失
        比如：向量对应图像，则我们可以对输入向量做些加减法
               戴眼镜的人-不戴眼镜的人+戴眼镜的人=戴眼镜的人
               （人很多，因为单个效果不好，所以多个做了个平均，所以生成
        
 3.图像翻译问题(Pix2Pix)       
    问题描述：对于某个点，会有多个表示，但是语义一样
        例如：蝴蝶 黑白，彩色，
              包包 素描，实物
              公路 白天，晚上，
              房子 卫星图，示意图    但是同个位置上的事物是不变的
    与DCGAN的区别：
        就是判断器D，不仅判断生成的是否是真实图片
                      还要，判断生成和原始输入是否是一对
    模型结构G：也是对比Endoder-decoder来，这里是使用U-Net
         Endoder-decoder: 先卷积使得图像变小，再反卷积恢复
            好处：1.使得中间的输出值变小，计算量就变小了，从而提高了计算速度
                  2.先变下再变大，使得输出层的结果都有较大的视野域，所以生成结果比较好
         为了提高图像分类效果：
            把卷积和对应的反卷积曾做一个拼接(两者大小一样)，也就是残差操作
    与DCGAN的区别：
        生成器G的输入变成了图像，这样就不能直接反卷积，而是先卷积再反卷积(U-Net)
 
 4. 非配对图像翻译(CycleGAN)
    (没有事先配成对，或者根本没有它对应的其他表示)
     Pix2Pix的数据是成对的，类似有监督数据
     目标：
        从非配对的图像中学到映射(比如，所有白天对应黑天)
        类比语言翻译：就是把句子翻译过去，还能再翻译回来，这就是翻译成功
     问题建模：
        设定两个领域，x,y
        则学习两个映射，x->y, y->x
        所以要学习两个判别器：DX 区分x和F(y)   DY 区分y和G(x)
        两个生成器
     CycleGAN:
        两套GAN，G和DY,  F和DX
        两个一致性损失：一种变成另一个中，再变换来，图像是不变的（这里是个双向过程，所以要两个损失
        最后得到两个映射，其实我们只要一个映射(一种到另一种，不让它再转回来)
     CycleGAN为啥会有效果？
        A->B是没有配对的，所以A可以映射到B中任何一张图像，但是有一致性损失的时候，就会降低
            或者约束了A映射到B所对应的空间就会变小(不是B中任何一张了)
     CycleGAN网络细节：
        生成器G与风格图像转换V2的transforrm net一样，可以替换为Pix2Pix的数据生成模型
        判别器D的输入为70*70的patch(无论多大的图都切分下)，好处输入小，计算就快
        训练稳定的loss: 给GAN的损失加个平方，训练会更加稳定
        Lambada=10
        Adam优化算法
        Learing_rate前100次为0.0002，100后线性下降
     CycleGAN 与  Pix2Pix 区别：
        数据是不匹配的，一致性损失，为了加强一致性所以用了两套
 
 5.多领域图像翻译(StarGAN)
        上述3种都是单任务(两个领域之间的转换)
        如果是三个领域之间的转换，用上面四种方法就要构建6个模型  4领域-》12个模型
        多任务，例如：更换头发颜色，表情，年龄    这样模型之间又不共享，就低效了。  为此可以设计为一个么？
    StarGAN       
        生成器G的输入添加目标领域信息(生成器添加多余的信息，用来指定生成那个目标领域的) 
        判别器D判断是否真实图像外，还需要判别类别(不能干看着G多加东西)
        图像重建loss
    特点：
        解决的是陈谷底图像问题
        生成器G中有领域信息
        判别器D中也要判断是否是某个领域
        G和D训练的时候目标函数不同
    